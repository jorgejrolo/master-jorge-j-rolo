{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNApSUkX9a15S3zF+WcQ7pG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jorgejrolo/master-jorge-j-rolo/blob/main/SF%E2%80%91HCU%E2%80%91Audit%E2%80%91GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Author:** Jorge J. Rolo  \n",
        "**Purpose:** Prioritize URLs by Helpful Content (HCU) risk using Screaming Frog exports.\n",
        "\n",
        "### Input\n",
        "- Screaming Frog export (`Internal_All.csv` or a custom export with columns: `Address`, `Title 1`, `Meta Description 1`, `Word Count`, `H1-1`, `H2-1`, `Status Code`, `Canonical Link Element 1`, `Inlinks`, `Outlinks`, `Indexability` …)\n",
        "\n",
        "### Output\n",
        "- A scored table with **HCU_Risk (0–100)** and explanations.\n",
        "- Shortlist of **High‑Impact Fixes**.\n",
        "- A prompt template to push the top 50 URLs to GPT for qualitative review.\n",
        "\n"
      ],
      "metadata": {
        "id": "K40gXQRp8Kes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = (10,5)\n",
        "print('Ready. Upload your Screaming Frog CSV in the next cell.')"
      ],
      "metadata": {
        "id": "DGzRMx8__ZGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    fn = list(uploaded.keys())[0]\n",
        "except Exception:\n",
        "    # Fallback: create a tiny sample\n",
        "    fn = None\n",
        "    sample = pd.DataFrame({\n",
        "        'Address':['/guide/rtx-4070','/blog/how-to-choose-psu','/category/gpu','/product/gpu-rtx-4070'],\n",
        "        'Title 1':['RTX 4070 Guide','Choose PSU','GPUs','RTX 4070 GPU'],\n",
        "        'Meta Description 1':['Full guide','Tips','Listing','Product'],\n",
        "        'Word Count':[420,1800,120,350],\n",
        "        'H1-1':['Guide','How to choose a PSU','GPUs','RTX 4070 GPU'],\n",
        "        'H2-1':['','Wattage & Efficiency','','Specs'],\n",
        "        'Status Code':[200,200,200,200],\n",
        "        'Canonical Link Element 1':['/guide/rtx-4070','','/category/gpu','/product/gpu-rtx-4070'],\n",
        "        'Inlinks':[8,42,120,33],\n",
        "        'Outlinks':[12,24,0,4],\n",
        "        'Indexability':['Indexable','Indexable','Indexable','Indexable']\n",
        "    })\n",
        "    sample.to_csv('/mnt/data/sample_sf.csv', index=False)\n",
        "    print('No file uploaded – using sample at /mnt/data/sample_sf.csv')\n",
        "    fn = '/mnt/data/sample_sf.csv'\n",
        "\n",
        "df = pd.read_csv(fn)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "9f8wbdZe_mnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['word_count'] = df.get('Word Count', 0).fillna(0).astype(int)\n",
        "df['has_h1'] = df.get('H1-1','').fillna('').str.len().gt(0)\n",
        "df['has_h2'] = df.get('H2-1','').fillna('').str.len().gt(0)\n",
        "df['is_indexable'] = df.get('Indexability','').fillna('').str.contains('Indexable', case=False)\n",
        "df['inlinks'] = df.get('Inlinks',0).fillna(0).astype(int)\n",
        "df['outlinks'] = df.get('Outlinks',0).fillna(0).astype(int)\n",
        "df['thin'] = df['word_count'] < 300\n",
        "df['orphanish'] = df['inlinks'] < 3\n",
        "df['no_structure'] = ~df['has_h1'] | ~df['has_h2']\n",
        "df['weak_meta'] = df.get('Meta Description 1','').fillna('').str.len().lt(80)\n",
        "df['dup_canonical'] = df['Canonical Link Element 1'].duplicated(keep=False)\n",
        "\n",
        "# HCU risk scoring (heuristic – tweak weights per site)\n",
        "score = (\n",
        "    df['thin'].astype(int)*25 +\n",
        "    df['orphanish'].astype(int)*15 +\n",
        "    df['no_structure'].astype(int)*15 +\n",
        "    df['weak_meta'].astype(int)*10 +\n",
        "    df['dup_canonical'].astype(int)*20 +\n",
        "    (~df['is_indexable']).astype(int)*15\n",
        ")\n",
        "df['HCU_Risk'] = score.clip(0,100)\n",
        "df[['Address','word_count','inlinks','has_h1','has_h2','HCU_Risk']].head()"
      ],
      "metadata": {
        "id": "aBKfQVCX_u-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax = df['HCU_Risk'].plot(kind='hist', bins=10)\n",
        "ax.set_title('HCU Risk Distribution')\n",
        "ax.set_xlabel('Risk (0-100)')\n",
        "plt.show()\n",
        "\n",
        "top = df.sort_values('HCU_Risk', ascending=False).head(25)\n",
        "top[['Address','HCU_Risk','word_count','inlinks','has_h1','has_h2']]"
      ],
      "metadata": {
        "id": "fLNXXoSz_36A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = df.sort_values('HCU_Risk', ascending=False)\n",
        "out_path = '/mnt/data/hcu_scored_urls.csv'\n",
        "out.to_csv(out_path, index=False)\n",
        "print('Exported to', out_path)"
      ],
      "metadata": {
        "id": "DUYI4nxj_-Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_prompt(rows):\n",
        "    tmpl = [\n",
        "        'You are auditing URLs for Helpful Content risk. For each, comment on usefulness, originality, structure, and what to fix first. Be concise.']\n",
        "    for _,r in rows.iterrows():\n",
        "        tmpl.append(f\"URL: {r['Address']} | words={r['word_count']} | inlinks={r['inlinks']} | H1={bool(r['has_h1'])} | H2={bool(r['has_h2'])} | risk={r['HCU_Risk']}\")\n",
        "    return '\\n'.join(tmpl)\n",
        "\n",
        "prompt_text = build_prompt(top)\n",
        "print(prompt_text[:1000] + ('...' if len(prompt_text)>1000 else ''))"
      ],
      "metadata": {
        "id": "xxHHYVKlAHss"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}